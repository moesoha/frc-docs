# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2019, FIRST
# This file is distributed under the same license as the FIRST Robotics
# Competition package.
# Soha Jin <soha@lohu.info>, 2019.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: FIRST Robotics Competition 2019\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2019-09-23 22:33+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.7.0\n"

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:2
msgid "Introduction to GRIP"
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:3
msgid ""
"GRIP is a tool for developing computer vision algorithms interactively "
"rather than through trial and error coding. After developing your "
"algorithm you may run GRIP in headless mode on your roboRIO, on a Driver "
"Station Laptop, or on a coprocessor connected to your robot network. With"
" Grip you choose vision operations to create a graphical pipeline that "
"represents the sequence of operations that are performed to complete the "
"vision algorithm."
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:8
msgid ""
"GRIP is based on OpenCV, one of the most popular computer vision software"
" libraries used for research, robotics, and vision algorithm "
"implementations. The operations that are available in GRIP are almost a 1"
" to 1 match with the operations available if you were hand coding the "
"same algorithm with some text-based programming language."
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:13
msgid "The GRIP user interface"
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:16
msgid "The GRIP user interface consists of 4 parts:"
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:18
msgid ""
"**Image Sources** are the ways of getting images into the GRIP pipeline. "
"You can provide images through attached cameras or files. Sources are "
"almost always the beginning of the image processing algorithm."
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:20
msgid ""
"**Operation Palette** contains the image processing steps from the OpenCV"
" library that you can chain together in the pipeline to form your "
"algorithm. Clicking on an operation in the palette adds it to the end of "
"the pipeline. You can then use the left and right arrows to move the "
"operation within the pipeline."
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:23
msgid ""
"**Pipeline** is the sequence of steps that make up the algorithm. Each "
"step (operation) in the pipeline is connected to a previous step from the"
" output of one step to an input to the next step. The data flows from "
"generally from left to right through the connections that you create."
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:26
msgid ""
"**Image Preview** are shows previews of the result of each step that has "
"it's preview button pressed. This makes it easy to debug algorithms by "
"being able to preview the outputs of each intermediate step."
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:30
msgid "Finding the yellow square"
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:33
msgid ""
"In this application we will try to find the yellow square in the image "
"and display it's position. The setup is pretty simple, just a USB web "
"camera connected to the computer looking down at some colorful objects. "
"The yellow plastic square is the thing that we're interested in locating "
"in the image."
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:38
msgid "Enable the image source"
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:41
msgid ""
"The first step is to acquire an image. To use the source, click on the "
"\"Add Webcam\" button and select the camera number. In this case the "
"Logitech USB camera that appeared as Webcam 0 and the computer monitor "
"camera was Webcam 1. The web camera is selected in this case to grab the "
"image behind the computer as shown in the setup. Then select the image "
"preview button and the real-time display of the camera stream will be "
"shown in the preview area."
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:47
msgid "Resize the image"
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:50
msgid ""
"In this case the camera resolution is too high for our purposes, and in "
"fact the entire image cannot even be viewed in the preview window. The "
"\"Resize\" operation is clicked from the Operation Palette to add it to "
"the end of the pipeline. To help locate the Resize operation, type "
"\"Resize\" into the search box at the top of the palette. The steps are:"
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:55
msgid "Type \"Resize\" into the search box on the palette"
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:56
msgid ""
"Click the Resize operation from the palette. It will appear in the "
"pipeline."
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:57
msgid ""
"Enter the x and y resize scale factor into the resize operation in the "
"pipeline. In this case 0.25 was chosen for both."
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:58
msgid ""
"Drag from the Webcam image output mat socket to the Resize image source "
"mat socket. A connection will be shown indicating that the camera output "
"is being sent to the resize input."
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:60
msgid ""
"Click on the destination preview button on the \"Resize\" operation in "
"the pipeline. The smaller image will be displayed alongside the larger "
"original image. You might need to scroll horizontally to see both as "
"shown."
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:62
msgid ""
"Lastly, click the Webcam source preview button since there is no reason "
"to look at both the large image and the smaller image at the same time."
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:66
msgid "Find only the yellow parts of the image"
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:69
msgid ""
"The next step is to remove everything from the image that doesn't match "
"the yellow color of the piece of plastic that is the object being "
"detected. To do that a HSV Threshold operation is chosen to set upper and"
" lower limits of HSV values to indicate which pixels should be included "
"in the resultant binary image. Notice that the target area is white while"
" everything that wasn't within the threshold values are shown in black. "
"Again, as before:"
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:74
msgid "Type HSV into the search box to find the HSV Threshold operation."
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:75
msgid ""
"Click on the operation in the palette and it will appear at the end of "
"the pipeline."
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:76
msgid ""
"Connect the dst (output) socket on the resize operation to the input of "
"the HSV Threshold."
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:77
msgid ""
"Enable the preview of the HSV Threshold operation so the result of the "
"operation is displayed in the preview window."
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:78
msgid ""
"Adjust the Hue, Saturation, and Value parameters only the target object "
"is shown in the preview window."
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:81
msgid "Get rid of the noise and extraneous hits"
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:84
msgid ""
"This looks pretty good so far, but sometimes there is noise from other "
"things that couldn't quite be filtered out. To illustrate one possible "
"technique to reduce those occasional pixels that were detected, an "
"Erosion operation is chosen. Erosion will remove small groups of pixels "
"that are not part of the area of interest."
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:89
msgid "Mask just the yellow area from the original image"
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:92
msgid ""
"Here a new image is generated by taking the original image and masking "
"(and operation) it with the the results of the erosion. This leaves just "
"the yellow card as seen in the original image with nothing else shown. "
"And it makes it easy to visualize exactly what was being found through "
"the series of filters."
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:97
msgid "Find the yellow area (blob)"
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:100
msgid ""
"The last step is actually detecting the yellow card using a Blob "
"Detector. This operation looks for a grouping of pixels that have some "
"minimum area. In this case, the only non-black pixels are from the yellow"
" card after the filtering is done. You can see that a circle is drawn "
"around the detected portion of the image. In the release version of GRIP "
"(watch for more updates between now and kickoff) you will be able to send"
" parameters about the detected blob to your robot program using Network "
"Tables."
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:106
msgid "Status of GRIP"
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:107
msgid ""
"As you can see from this example, it is very easy and fast to be able to "
"do simple object recognition using GRIP. While this is a very simple "
"example, it illustrates the basic principles of using GRIP and feature "
"extraction in general. Over the coming weeks the project team will be "
"posting updates to GRIP as more features are added. Currently it supports"
" cameras (Axis ethernet camera and web cameras) and image inputs. There "
"is no provision for output yet although Network Tables and ROS (Robot "
"Operating System) are planned."
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:113
msgid ""
"You can either download a pre-built release of the code from the github "
"page \"Releases\" section (https://github.com/WPIRoboticsProjects/GRIP) "
"or you can clone the source repository and built it yourself. Directions "
"on building GRIP are on the project page. There is also additional "
"documentation on the project wiki."
msgstr ""

#: ../../source/docs/software/vision-processing/grip/introduction-to-grip.rst:117
msgid ""
"So, please play with GRiP and give us feedback here on the forum. If you "
"find bugs, you can either post them here or as a Github project issue on "
"the project page."
msgstr ""

